\begin{rSection}{Publications}

{\bf Journal}
\vspace{-0.15cm}
\begin{enumerate}
    \item \underline{{\bf Taichi Nishimura}}, Shota Nakada, Masayoshi Kondo, ``Vision-Language Models Learn Super Images for Efficient Partially Relevant Video Retrieval'', ACM Transactions on Multimedia Computing, Communications, and Applications (ACM TOMM), 2024.
    \item \underline{{\bf Taichi Nishimura}}, Atsushi Hashimoto, Yoshitaka Ushiku, Hirotaka Kameko, and Shinsuke Mori, ``Recipe Generation from Unsegmented Cooking Videos'', ACM Transactions on Multimedia Computing, Communications, and Applications (ACM TOMM), 2024.
    \item \underline{{\bf Taichi Nishimura}}, Atsushi Hashimoto, Yoshitaka Ushiku, Hirotaka Kameko, and Shinsuke Mori, ``State-aware Video Procedural Captioning'', Multimedia Tools and Applications (MTAP), 2023.
    \item \underline{{\bf Taichi Nishimura}}, Kojiro Sakoda, Atsushi Ushiku, Atsushi Hashimoto, Natsuko Okuda, Fumihito Ono, Hirotaka Kameko, and Shinsuke Mori, ``BioVL2: An Egocentric Biochemical Video-and-Language Dataset'', Journal of Natural Language Processing, 2023.
    \item \underline{{\bf Taichi Nishimura}}, Atsushi Hashimoto, Yoshitaka Ushiku, Hirotaka Kameko, Yoko Yamakata, and Shinsuke Mori, ``Structure-Aware Procedural Text Generation from an Image Sequence'', IEEE Access, Vol. 9, 2020.
    \item \underline{{\bf Taichi Nishimura}}, Atsushi Hashimoto, and Shinsuke Mori, ``Recipe Generation from a Photo Sequence by Focusing on Verbalizing Important Terms'', Journal of Natural Language Processing, 2020.
\end{enumerate}

{\bf International Conference}
\vspace{-0.15cm}
\begin{enumerate}
    \item Hokuto Munakata, \underline{{\bf Taichi Nishimura}}, Shota Nakada, Tatsuya Komatsu, ``Language-based Audio Moment Retrieval'', IEEE International Conference on Acoustic, Speech, and Signal Processing (ICASSP25), 2025.
    \item Shota Nakada, \underline{{\bf Taichi Nishimura}}, Hokuto Munakata, Tatsuya Komatsu, ``DETECLAP: Enhancing Audio-Visual Representation Learning with Object Information'', IEEE International Conference on Acoustic, Speech, and Signal Processing (ICASSP25), 2025.
    \item Takehiko Ohkawa, Takuma Yagi, \underline{{\bf Taichi Nishimura}}, Ryosuke Furuta, Atsushi Hashimoto, Yoshitaka Ushiku, Yoichi Sato, ``Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos'', IEEE/CVF Winter Conference on Applications of Computer Vision (WACV25), 2025.
    \item \underline{{\bf Taichi Nishimura}}, Shota Nakada, Hokuto Munakata, Tatsuya Komatsu, ``Lighthouse: A User-Friendly Library for Reproducible Video Moment Retrieval and Highlight Detection'', In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing - System Demonstration Track (EMNLP24 Demo), 2024.
    \item Hokuto Munakata, \underline{{\bf Taichi Nishimura}}, Shota Nakada, Tatsuya Komatsu, ``Pre-trained models, Datasets, Data Augmentation, and Inference Time Augmentation for Language-based Audio Retrieval'', IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE Workshop) 2024.
    \item Hokuto Munakata, \underline{{\bf Taichi Nishimura}}, Shota Nakada, Tatsuya Komatsu, ``Training Strategy of Massive Text-to-Audio Models and GPT-based Query-Augmentation'', IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events Challenge Task 8: Language-Based Audio Retrieval (4th) (DCASE Challenge) 2024.
    \item Takehiko Ohkawa, Takuma Yagi, \underline{{\bf Taichi Nishimura}}, Ryosuke Furuta, Atsushi Hashimoto, Yoshitaka Ushiku, Yoichi Sato, ``Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos'', The 1st Workshop on Learning from Procedural Videos and Language in conjunction with CVPR2024 (LPVL24), 2024.
    \item Keyaki Ohno, Hirotaka Kameko, Keisuke Shirai, \underline{{\bf Taichi Nishimura}}, and Shinsuke Mori ``Automatic Construction of a Large-Scale Corpus for Geoparsing Using Wikipedia Hyperlinks'', In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING), 2024.
    \item Keisuke Shirai, Atsushi Hashimoto, \underline{{\bf Taichi Nishimura}}, Hirotaka Kameko, Shuhei Kurita, Yoshitaka Ushiku and Shinsuke Mori ``Visual Recipe Flow: A Dataset for Learning Visual State Changes of Objects with Recipe Flows'', In Proceedings of the 29th International Conference on Computational Linguistics (COLING), 2022.
    \item \underline{{\bf Taichi Nishimura}}, Katushiko Ishiguro, Keita Higuchi, and Masaaki Kotera ``Multimodal Dish Pairing: Predicting Side Dishes to Serve with a Main Dish'', In Proceedings of the 1st International Workshop on Multimedia for Cooking, Eating, and related APPlications 2022 in conjunction with ACMMM2022 (CEA++), 2022 \textcolor{red}{(Best Paper Award)}.
    \item Sara Ozeki, Katushiko Ishiguro, Masaaki Kotera, \underline{{\bf Taichi Nishimura}}, and Keita Higuchi ``Recipe Recommendation for Balancing Ingredient Preference and Daily Nutrients'', In Proceedings of the 1st International Workshop on Multimedia for Cooking, Eating, and related APPlications 2022 in conjunction with ACMMM2022 (CEA++), 2022.
    \item Jieyong Zhu, \underline{{\bf Taichi Nishimura}}, Makoto Goto, and Shinsuke Mori ``Multimedia Retrieval of Historical Materials'', Digital Humanities (DH), 2022.
    \item Atsushi Hashimoto, \underline{{\bf Taichi Nishimura}}, Yoshitaka Ushiku, Hirotaka Kameko, and Shinsuke Mori ``Cross-modal Representation Learning for Understanding Manufacturing Procedure'', The 23th International Conference on Human-Computer Interaction (HCII), 2022.
    \item Kento Tanaka, \underline{{\bf Taichi Nishimura}}, Hiroaki Nanjo, Keisuke Shirai, Hirotaka Kameko, and Masatake Dantsuji ``Image Description Dataset for Language Learners'', The 13th International Conference on Language Resources and Evaluation (LREC), 2022.
    \item \underline{{\bf Taichi Nishimura}}, Atsushi Hashimoto, Yoshitaka Ushiku and Shinsuke Mori, ``State-aware Video Procedural Captioning'', In Proceedings of the 29th ACM International Conference on Multimedia (ACMMM), 2021.
    \item \underline{{\bf Taichi Nishimura}}, Kojiro Sakoda, Atsushi Hashimoto, Yoshitaka Ushiku, Natsuko Tanaka, Fumihito Ono, Hirotaka Kameko, and Shinsuke Mori, ``Egocentric Biochemical Video-and-Language Dataset'', In Proceedings of the 4th Workshop on Closing the Loop Between Vision and Language in conjunction with ICCV (CLVL), 2021.
    \item \underline{{\bf Taichi Nishimura}}, Suzushi Tomori, Hayato Hashimoto, Atsushi Hashimoto, Yoko Yamakata, Jun Harashima, Yoshitaka Ushiku and Shinsuke Mori, ``Visual Grounding Annotation of Recipe Flow Graph'', In Proceedings of the 12th International Conference on Language Resources and Evaluation (LREC), 2020.
    \item \underline{{\bf Taichi Nishimura}}, Atsushi Hashimoto, Shinsuke Mori, ``Procedural Text Generation from a Photo Sequence'', In Proceedings of 
the International Conference on Natural Language Generation (INLG), 2019.
    \item \underline{{\bf Taichi Nishimura}}, Atsushi Hashimoto, Yoko Yamakata, Shinsuke Mori, ``Frame Selection for Producing Recipe with Pictures from an Execution Video of a Recipe'', In Proceedings of the Workshop on Multimedia for Cooking and Eating Activities (CEA) in conjunction with ICMR, 2019 \textcolor{red}{(Best Paper Award)}.
\end{enumerate}

{\bf Preprint (Under review)}
\vspace{-0.15cm}
\begin{enumerate}
    \item \underline{{\bf Taichi Nishimura}}, Koki Yamamoto, Yuto Haneji, Keiya Kajimura, Chihiro Nishiwaki, Eriko Daikoku, Natsuko Okuda, Fumihito Ono, Hirotaka Kameko, Shinsuke Mori, ``BioVL-QR: Egocentric Biochemical Video-and-Language Dataset Using Micro QR Codes'', arXiv, 2024.
    \item Tomoya Yoshida, Shuhei Kurita, \underline{{\bf Taichi Nishimura}}, Shinsuke Mori, ``Text-driven Affordance Learning from Egocentric Vision'', arXiv, 2024.
    \item \underline{{\bf Taichi Nishimura}}, Shota Nakada, Masayoshi Kondo, ``On the Audio Hallucinations in Large Video-Audio Language Models'', arXiv, 2024.
\end{enumerate}

\end{rSection}